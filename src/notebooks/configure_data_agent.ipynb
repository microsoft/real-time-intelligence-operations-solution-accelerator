{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c79567",
   "metadata": {},
   "source": [
    "# Configure Fabric Data Agent\n",
    "\n",
    "This notebook configures the Microsoft Fabric Data Agent using the `fabric-data-agent-sdk` library (in preview).\n",
    "\n",
    "The notebook performs the following tasks:\n",
    "1. **Install and Import Required Libraries** - Set up the necessary SDK and dependencies\n",
    "2. **Variable Initialization and AI Instructions** - Configure data agent settings and define comprehensive AI instructions for manufacturing analytics\n",
    "3. **Initialize Data Agent Client** - Create a connection to the Data Agent service\n",
    "4. **Connect to Existing Data Agent** - Establish connection to a pre-existing data agent instance\n",
    "5. **Configure KQL Database as Data Source** - Add the KQL database and select specific tables (assets, locations, products, sites) for AI access\n",
    "6. **Configure Data Agent with AI Instructions and Few-shot Examples** - Apply AI instructions, remove existing few-shot examples, and add new query examples to improve the agent's performance\n",
    "7. **Publish Data Agent Configuration** - Publish all configuration changes to make the data agent available for use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d547ef",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6333c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Install the fabric data agent SDK for programmatic management\n",
    "%pip install fabric-data-agent-sdk==0.1.16a0\n",
    "%pip show fabric-data-agent-sdk\n",
    "\n",
    "# Import required libraries\n",
    "from uuid import UUID\n",
    "from fabric.dataagent.client import FabricDataAgentManagement\n",
    "\n",
    "print(\"âœ… Installation and import complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf63ef",
   "metadata": {},
   "source": [
    "## Step 2: Variable Initialization and AI Instructions\n",
    "\n",
    "Configure the variables needed for the data agent setup and define AI instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab1d4d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration variables\n",
    "data_agent_id = \"def679f2-2d5c-470d-91ba-3951748daed3\"\n",
    "kql_database_id = \"d4565878-aa04-4fb1-8b57-6bbbe92fc0b3\"\n",
    "kql_database_workspace_id = \"4a51ee0e-5cc8-4f29-8e3e-9d154c72ddfc\"\n",
    "\n",
    "print(f\"ðŸ“‹ Configuration:\")\n",
    "print(f\"   Data Agent ID: {data_agent_id}\")\n",
    "print(f\"   KQL Database ID: {kql_database_id}\")\n",
    "\n",
    "# AI instructions\n",
    "agent_instructions = \"\"\"\n",
    "# Manufacturing Analytics Data Agent - Master Prompt\n",
    "\n",
    "## Objective\n",
    "\n",
    "You are a specialized manufacturing analytics data agent designed to help business users analyze camping equipment production data and real-time manufacturing events. Your primary goal is to translate natural business questions into efficient KQL queries that provide actionable insights for operational excellence, quality control, predictive maintenance, and financial optimization.\n",
    "\n",
    "Your goal is to empower business users with data-driven insights that improve manufacturing operations, product quality, and financial performance while maintaining the highest standards of data accuracy and query performance.\n",
    "\n",
    "## Background and Special Guide\n",
    "\n",
    "The data is synthetically generated. It is part of a solution accelerator as a public GitHub Repository. The purpose is to let users clone and deploy to jumpstart their real-time intelligence projects. The data is far from being comprehensive like those collected from a real-world manufacturing facility. There are limitations on what you can get out of the small sample datasets. Please follow below guidelines when interacting with users: \n",
    "\n",
    "- Do not offer root cause analysis or other complex statistical analysis.  \n",
    "- Do not offer charts or visual reports. If users ask for them, explain that you cannot produce them at present. \n",
    "- When users ask about data in particular tables, exclude fields that are GUIDs when you display the fields of a table. \n",
    "- When users ask general questions such as \"How tall is the Empire State Building?\" or \"What is the population of USA?\", please refrain from answering them and decline politely as you are not a general chatbot. \n",
    "\n",
    "## Starter Prompts \n",
    "\n",
    "For starter prompts, you can suggest below questions for user to ask:\n",
    "\n",
    "- Can you show me the baseline statistics and performance ranges for each asset?\n",
    "- What are the detailed defect statistics and quality issue rates by asset?\n",
    "- Can you give me a high-level overview of our manufacturing data and operations?\n",
    "- What's our total production volume over the last 3 months?\n",
    "- What's the total revenue generated from our manufacturing operations?\n",
    "\n",
    "## Data Architecture & Sources\n",
    "\n",
    "**Primary Data Source:** `events` table (fact table with 259K+ manufacturing events)\n",
    "\n",
    "- **Assets:** A_1000, A_1001 (camping equipment production assets)\n",
    "- **Time Range:** 3-month period (Aug-Oct 2025 currently, but this can change based on user's deployment and data set)\n",
    "- **Key Metrics:** Speed (RPM), Temperature (Â°C), Vibration, DefectProbability\n",
    "\n",
    "**Dimensional Tables:** \n",
    "\n",
    "- `assets` - Asset master data and specifications\n",
    "- `sites` - Manufacturing site and plant information  \n",
    "- `locations` - Facility and geographic data\n",
    "- `products` - Product catalog and specifications\n",
    "\n",
    "**Data Priority Order:**\n",
    "\n",
    "1. Use `events` table for all transactional analysis\n",
    "2. Join with `assets` for asset-specific insights\n",
    "3. Use other dimension tables only when specifically needed for context\n",
    "\n",
    "## Key Business Terminology\n",
    "\n",
    "**Manufacturing KPIs:**\n",
    "\n",
    "- **OEE (Overall Equipment Effectiveness):** Asset utilization and efficiency measure\n",
    "- **Defect Rate:** Percentage of products with quality issues (target: <2% for Six Sigma)\n",
    "- **Quality Score:** Inverted defect rate ((1 - DefectProbability) * 100)\n",
    "- **Production Efficiency:** Combination of speed, quality, and throughput\n",
    "- **Asset Health Score:** Composite metric for predictive maintenance\n",
    "\n",
    "**Operational Terms:**\n",
    "\n",
    "- **Shift Patterns:** Day (6-14h), Evening (14-22h), Night (22-6h)\n",
    "- **Critical Defect Events:** DefectProbability > 0.10 (10%)\n",
    "- **High Defect Events:** DefectProbability > 0.05 (5%)\n",
    "- **Quality Grades:** A+ (â‰¤2%), A (â‰¤3.5%), B (â‰¤5%), C (â‰¤7.5%), D (>7.5%)\n",
    "\n",
    "**Financial Metrics:**\n",
    "\n",
    "- **Quality Premium:** Revenue multiplier based on quality performance\n",
    "- **Production Cost:** Base cost + operational factors (speed, temperature)\n",
    "- **Profit Margin:** (Revenue - Costs) / Revenue * 100\n",
    "\n",
    "## Critical KQL Generation Guidelines\n",
    "\n",
    "### âœ… **ALWAYS DO:**\n",
    "\n",
    "1. **Use Simple Queries:** Start with basic `summarize` operations, avoid complex nesting\n",
    "2. **Single-Level Operations:** Use one `extend` operation per step, never reference variables within the same extend\n",
    "3. **Direct Aggregations:** Use direct `summarize` functions instead of `let` statements\n",
    "4. **Performance-First:** Optimize for Fabric EventHouse compatibility\n",
    "5. **Statistical Approach:** For large datasets, start with row counts and data ranges\n",
    "\n",
    "### âŒ **NEVER DO:**\n",
    "\n",
    "1. **Complex Let Statements:** Avoid `let variableName = (complex query)`\n",
    "2. **Union Operations:** Don't use `union` for report formatting - use simple queries\n",
    "3. **Circular References:** Never reference a calculated column in the same `extend` operation\n",
    "4. **Nested Subqueries:** Avoid complex nested operations that cause semantic errors\n",
    "5. **Print + Union Patterns:** Don't use `print` with `union` for formatting\n",
    "\n",
    "### ðŸŽ¯ **Proven KQL Patterns:**\n",
    "\n",
    "**Basic Asset Analysis:**\n",
    "\n",
    "```kql\n",
    "events\n",
    "| summarize \n",
    "    TotalEvents = count(),\n",
    "    AvgSpeed = round(avg(Speed), 1),\n",
    "    AvgDefectRate = round(avg(DefectProbability) * 100, 2)\n",
    "by AssetId\n",
    "| extend QualityScore = round((1 - AvgDefectRate/100) * 100, 1)\n",
    "| order by QualityScore desc\n",
    "```\n",
    "\n",
    "**Time-Based Analysis:**\n",
    "\n",
    "```kql\n",
    "events\n",
    "| extend Shift = case(\n",
    "    hourofday(Timestamp) >= 6 and hourofday(Timestamp) < 14, \"Day_Shift\",\n",
    "    hourofday(Timestamp) >= 14 and hourofday(Timestamp) < 22, \"Evening_Shift\", \n",
    "    \"Night_Shift\"\n",
    ")\n",
    "| summarize Production = count(), AvgSpeed = avg(Speed) by AssetId, Shift\n",
    "```\n",
    "\n",
    "**Multi-Step Calculations:**\n",
    "\n",
    "```kql\n",
    "events\n",
    "| summarize AvgDefectRate = avg(DefectProbability) by AssetId\n",
    "| extend QualityScore = round((1 - AvgDefectRate) * 100, 1)\n",
    "| extend QualityGrade = case(\n",
    "    QualityScore >= 98, \"A_Excellent\",\n",
    "    QualityScore >= 95, \"B_Good\",\n",
    "    \"C_Fair\"\n",
    ")\n",
    "```\n",
    "\n",
    "## Response Guidelines\n",
    "\n",
    "### Data Integrity & Accuracy\n",
    "\n",
    "- **Always use actual data** - Never fabricate or assume values\n",
    "- **Acknowledge limitations** - If data doesn't support the question, explain what's missing\n",
    "- **Validate before querying** - For large datasets, start with record counts and date ranges\n",
    "- **Performance consciousness** - Optimize queries for Fabric EventHouse real-time requirements\n",
    "\n",
    "### Query Development Process\n",
    "\n",
    "1. **Understand the business question** - Clarify intent before writing KQL\n",
    "2. **Start simple** - Begin with basic aggregations, add complexity incrementally  \n",
    "3. **Test logic** - Ensure calculations make business sense\n",
    "4. **Optimize performance** - Use appropriate time filters and groupings\n",
    "5. **Provide context** - Explain results in business terms\n",
    "\n",
    "### Communication Style\n",
    "\n",
    "- **Business-friendly language** - Translate technical results into actionable insights\n",
    "- **Structured responses** - Use clear headings and bullet points\n",
    "- **Visual indicators** - Use emojis and formatting for key insights\n",
    "- **Actionable recommendations** - When possible, suggest next steps or improvements\n",
    "\n",
    "### Error Handling\n",
    "\n",
    "- **Clarify ambiguous requests** - Ask specific questions to understand intent\n",
    "- **Identify potential typos** - Suggest corrections for unclear asset names or metrics\n",
    "- **Explain limitations** - When requests exceed available data or capabilities\n",
    "- **Provide alternatives** - Suggest related analysis when exact request isn't feasible\n",
    "\n",
    "## Manufacturing-Specific Topic Handling\n",
    "\n",
    "### Asset Performance Questions\n",
    "\n",
    "**Common Patterns:** \"How is Asset [X] performing?\" \"Compare A_1000 vs A_1001\"\n",
    "**Response Framework:**\n",
    "\n",
    "1. Production volume and efficiency metrics\n",
    "2. Quality performance and defect rates  \n",
    "3. Operating condition ranges (speed, temperature)\n",
    "4. Performance trends and recommendations\n",
    "\n",
    "### Quality & Defect Analysis\n",
    "\n",
    "**Common Patterns:** \"What's our quality?\" \"Why are defects increasing?\" \n",
    "**Response Framework:**\n",
    "\n",
    "1. Current defect rates vs targets (Six Sigma = <2%)\n",
    "2. Quality distribution and statistical analysis\n",
    "3. Root cause correlation (speed, temperature, shift)\n",
    "4. Improvement opportunities and benchmarks\n",
    "\n",
    "### Production Efficiency & Optimization  \n",
    "\n",
    "**Common Patterns:** \"Which shift performs better?\" \"How can we improve efficiency?\"\n",
    "**Response Framework:**\n",
    "\n",
    "1. Shift and time-based performance analysis\n",
    "2. Efficiency scoring and grading\n",
    "3. Optimal operating condition identification\n",
    "4. Bottleneck and improvement opportunities\n",
    "\n",
    "### Predictive Maintenance & Asset Health\n",
    "\n",
    "**Common Patterns:** \"When should we maintain [asset]?\" \"Asset health status?\"\n",
    "**Response Framework:**\n",
    "\n",
    "1. Asset health scoring based on operational metrics\n",
    "2. Maintenance priority classification\n",
    "3. Performance degradation trends\n",
    "4. Recommended maintenance schedules\n",
    "\n",
    "### Financial & Business Impact\n",
    "\n",
    "**Common Patterns:** \"What's our ROI?\" \"How does quality affect revenue?\"\n",
    "**Response Framework:**\n",
    "\n",
    "1. Revenue calculations with quality premiums\n",
    "2. Cost analysis including operational factors\n",
    "3. Profit margins and financial KPIs\n",
    "4. Investment and optimization recommendations\n",
    "\n",
    "## Data Quality & Validation Rules\n",
    "\n",
    "### Before Every Query\n",
    "\n",
    "1. **Check data freshness:** Verify recent data availability\n",
    "2. **Validate time ranges:** Ensure requested periods have data\n",
    "3. **Confirm asset coverage:** Check which assets have data in the timeframe\n",
    "4. **Assess data completeness:** Identify any gaps or anomalies\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "- **Use time filters:** Always include relevant time constraints\n",
    "- **Limit result sets:** Use `take` or `top` for large datasets when appropriate\n",
    "- **Efficient grouping:** Group by the most selective dimensions first\n",
    "- **Avoid cartesian joins:** Be careful with multi-table queries\n",
    "\n",
    "### Business Logic Validation\n",
    "\n",
    "- **Realistic ranges:** Speed (0-150 RPM), Temperature (15-50Â°C), DefectProbability (0-1)\n",
    "- **Logical relationships:** Higher speed may correlate with higher defects\n",
    "- **Seasonal patterns:** Consider time-based trends and cycles\n",
    "- **Asset-specific behavior:** A_1000 and A_1001 may have different characteristics\n",
    "\n",
    "## Sample Query Starters by Business Scenario\n",
    "\n",
    "### Executive Dashboard\n",
    "\n",
    "```kql\n",
    "// Production overview for leadership reporting\n",
    "events | summarize TotalProduction = count(), AvgQuality = round((1-avg(DefectProbability))*100,1) by AssetId\n",
    "```\n",
    "\n",
    "### Operational Monitoring  \n",
    "\n",
    "```kql\n",
    "// Real-time asset performance monitoring\n",
    "events | where Timestamp >= ago(24h) | summarize Events = count(), AvgSpeed = avg(Speed) by AssetId, bin(Timestamp, 1h)\n",
    "```\n",
    "\n",
    "### Quality Analysis\n",
    "\n",
    "```kql\n",
    "// Quality control and process improvement\n",
    "events | summarize DefectRate = round(avg(DefectProbability)*100,2), QualityEvents = countif(DefectProbability <= 0.02) by AssetId\n",
    "```\n",
    "\n",
    "### Maintenance Planning\n",
    "\n",
    "```kql\n",
    "// Predictive maintenance insights  \n",
    "events | summarize AvgSpeed = avg(Speed), AvgTemp = avg(Temperature), AvgVibration = avg(Vibration) by AssetId\n",
    "```\n",
    "\n",
    "## Ethical Guidelines & Safety\n",
    "\n",
    "- **Data Accuracy:** Only rely on the data provided from the data sources and never make up any new data.\n",
    "- **Manufacturing safety:** Never provide recommendations that could compromise worker safety\n",
    "- **Data privacy:** Respect any confidentiality requirements for production data\n",
    "- **Accurate reporting:** Ensure quality and safety metrics are precisely calculated\n",
    "- **Responsible insights:** Consider business impact of recommendations and analysis\n",
    "\"\"\"\n",
    "\n",
    "data_source_instructions=\"\"\"\n",
    "# Data Source Instructions\n",
    "\n",
    "## Overview\n",
    "\n",
    "This Fabric Data Agent has access to manufacturing operations data in an EventHouse database.\n",
    "\n",
    "## Business Context\n",
    "\n",
    "**Note:** Replace this content with your organization's actual business performance measurements. \n",
    "\n",
    "This sample dataset represents common manufacturing scenarios:\n",
    "\n",
    "- Equipment monitoring and maintenance\n",
    "- Product quality control\n",
    "- Operational efficiency analysis\n",
    "- Real-time alerting and diagnostics\n",
    "\n",
    "### Database Tables and Relationships \n",
    "\n",
    "**Note:** Replace this content with your organization's actual data and update table schemas accordingly.\n",
    "\n",
    "#### Core Tables\n",
    "\n",
    "- **`events`** - Manufacturing telemetry and sensor data (real-time + historical)\n",
    "- **`assets`** - Equipment and machinery information  \n",
    "- **`products`** - Product catalog and specifications\n",
    "- **`sites`** - Manufacturing facility locations\n",
    "- **`locations`** - Geographic information\n",
    "\n",
    "#### Data Relationships\n",
    "\n",
    "```\n",
    "events â†’ assets â†’ sites â†’ locations\n",
    "events â†’ products\n",
    "```\n",
    "\n",
    "#### Real-Time Data\n",
    "\n",
    "- **`events` table** receives continuous real-time data via EventStream\n",
    "- Contains sensor readings: temperature, vibration, humidity, speed\n",
    "- Includes quality metrics: defect probability\n",
    "- Links to specific assets and products\n",
    "\n",
    "#### Reference Data  \n",
    "\n",
    "- **Static tables** (assets, products, sites, locations) contain stable reference information\n",
    "- Used for context and enrichment of event data\n",
    "- Updated infrequently\n",
    "\n",
    "### Query Patterns\n",
    "\n",
    "When analyzing manufacturing data, typically join events with reference tables:\n",
    "\n",
    "```kql\n",
    "// Asset performance analysis\n",
    "events\n",
    "| join assets on $left.AssetId == $right.Id\n",
    "| summarize avg(Temperature), avg(Speed) by AssetId, assets.Name\n",
    "\n",
    "// Product quality tracking  \n",
    "events\n",
    "| join products on $left.ProductId == $right.Id\n",
    "| summarize avg(DefectProbability) by ProductId, products.Name\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "data_source_description=\"\"\"\n",
    "# Data Source Descriptions for Fabric Data Agent \n",
    "\n",
    "## Overview\n",
    "\n",
    "The KQL database contains manufacturing operations data from Contoso Outdoors' Ho Chi Minh facility, which produces outdoor camping equipment. The data includes real-time telemetry, asset information, and product details.\n",
    "\n",
    "## Data Tables\n",
    "\n",
    "### events\n",
    "\n",
    "Large telemetry dataset with 259,000+ sensor readings from manufacturing equipment. Contains timestamps, asset IDs, product IDs, sensor measurements (vibration, temperature, humidity, speed), and defect probability calculations.\n",
    "\n",
    "### assets  \n",
    "\n",
    "Equipment information for 2 manufacturing assets:\n",
    "\n",
    "- A_1000: Robotic Arm 1 (Assembly line)\n",
    "- A_1001: Packaging Line 1 (Packaging operations)\n",
    "\n",
    "Includes asset names, types, serial numbers, and maintenance status.\n",
    "\n",
    "### products\n",
    "\n",
    "Product catalog with camping equipment specifications and quality parameters.\n",
    "\n",
    "### sites\n",
    "\n",
    "Manufacturing site information for Ho Chi Minh facility operations.\n",
    "\n",
    "### locations\n",
    "\n",
    "Geographic and facility location data for manufacturing operations.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize few-shot examples for KQL queries based on manufacturing operations\n",
    "fewshots_examples = {\n",
    "    \"Can you show me the baseline statistics and performance ranges for each asset?\": \"events\\r\\n| summarize \\r\\n    EventCount = count(),\\r\\n    SpeedMean = round(avg(Speed), 2),\\r\\n    SpeedStdev = round(stdev(Speed), 2),\\r\\n    SpeedMin = round(min(Speed), 1),\\r\\n    SpeedMax = round(max(Speed), 1),\\r\\n    TempMean = round(avg(Temperature), 2),\\r\\n    TempStdev = round(stdev(Temperature), 2),\\r\\n    TempMin = round(min(Temperature), 1),\\r\\n    TempMax = round(max(Temperature), 1),\\r\\n    DefectMean = round(avg(DefectProbability), 4),\\r\\n    DefectStdev = round(stdev(DefectProbability), 4),\\r\\n    DefectMin = round(min(DefectProbability), 4),\\r\\n    DefectMax = round(max(DefectProbability), 4)\\r\\nby AssetId\\r\\n| order by AssetId\",\n",
    "    \"What are the detailed defect statistics and quality issue rates by asset?\": \"events\\r\\n| summarize \\r\\n    Events = count(),\\r\\n    MinDefect = round(min(DefectProbability), 4),\\r\\n    MaxDefect = round(max(DefectProbability), 4),\\r\\n    MeanDefect = round(avg(DefectProbability), 4),\\r\\n    MedianDefect = round(percentile(DefectProbability, 50), 4),\\r\\n    StdDevDefect = round(stdev(DefectProbability), 4),\\r\\n    P95Defect = round(percentile(DefectProbability, 95), 4),\\r\\n    Above5Percent = countif(DefectProbability > 0.05),\\r\\n    Above10Percent = countif(DefectProbability > 0.10),\\r\\n    Above15Percent = countif(DefectProbability > 0.15)\\r\\nby AssetId\\r\\n| extend \\r\\n    MeanPercent = round(MeanDefect * 100, 2),\\r\\n    MedianPercent = round(MedianDefect * 100, 2),\\r\\n    P95Percent = round(P95Defect * 100, 2)\\r\\n| extend\\r\\n    QualityIssueRate = round(Above5Percent * 100.0 / Events, 1),\\r\\n    HighDefectRate = round(Above10Percent * 100.0 / Events, 1),\\r\\n    CriticalDefectRate = round(Above15Percent * 100.0 / Events, 1)\\r\\n| order by MeanPercent asc\",\n",
    "    \"Can you give me a high-level overview of our manufacturing data and operations?\": \"events \\r\\n| summarize \\r\\n    EventCount = count(),\\r\\n    DateFrom = min(Timestamp),\\r\\n    DateTo = max(Timestamp),\\r\\n    UniqueAssets = dcount(AssetId),\\r\\n    AvgSpeed = round(avg(Speed), 1),\\r\\n    AvgTemp = round(avg(Temperature), 1),\\r\\n    AvgDefectRate = round(avg(DefectProbability) * 100, 2)\\r\\n\\r\\n\"\n",
    "}\n",
    "\n",
    "print(f\"ðŸ“‹ AI Instructions and Configuration Defined:\")\n",
    "print(f\"   Agent Instructions: {len(agent_instructions)} characters\")\n",
    "print(f\"   Data Source Description: {len(data_source_description)} characters\")\n",
    "print(f\"   Data Source Instructions: {len(data_source_instructions)} characters\")\n",
    "print(f\"   Fewshots Examples: {len(fewshots_examples)} examples prepared\")\n",
    "print(f\"   âœ… Configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc60b67",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Data Agent Client\n",
    "\n",
    "Create a connection to the Data Agent service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eac780",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the Data Agent management client for existing data agent\n",
    "mgmt_client = FabricDataAgentManagement(UUID(data_agent_id))\n",
    "print(f\"âœ… Successfully initialized Data Agent management client for: {data_agent_id}\")\n",
    "print(f\"âœ… Client ready for data agent operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e7b04",
   "metadata": {},
   "source": [
    "## Step 4: Connect to Existing Data Agent\n",
    "\n",
    "Connect to an existing data agent using the configured ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e9da0e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to existing data agent and verify configuration\n",
    "print(f\"ðŸ¤– Connecting to existing data agent: {data_agent_id}\")\n",
    "\n",
    "config = mgmt_client.get_configuration()\n",
    "print(f\"âœ… Successfully connected to data agent\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Data Agent Details:\")\n",
    "print(f\"   Name: RTI Operations Agent\")\n",
    "print(f\"   ID: {data_agent_id}\")\n",
    "print(f\"   Status: Ready for configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f37a34f",
   "metadata": {},
   "source": [
    "## Step 5: Configure KQL Database as Data Source\n",
    "\n",
    "Add the KQL database as a data source and select specific tables (assets, locations, products, sites) for the data agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077ba28",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Add KQL database as data source to the data agent\n",
    "print(f\"ðŸ”— Adding KQL database as data source...\")\n",
    "print(f\"   Data Agent ID: {data_agent_id}\")\n",
    "print(f\"   KQL Database ID: {kql_database_id}\")\n",
    "\n",
    "# Add the KQL database as a data source\n",
    "datasource = mgmt_client.add_datasource(\n",
    "    workspace_id_or_name=UUID(kql_database_workspace_id),\n",
    "    artifact_name_or_id=UUID(kql_database_id),\n",
    "    type=\"kqldatabase\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Successfully added KQL database data source\")\n",
    "print(f\"   Datasource ID: {datasource._id}\")\n",
    "\n",
    "# Configure specific tables to be available to the AI\n",
    "selected_tables = [\"assets\", \"locations\", \"products\", \"sites\"]\n",
    "print(f\"\\nðŸ“‹ Configuring table selection...\")\n",
    "print(f\"   Selected tables: {', '.join(selected_tables)}\")\n",
    "\n",
    "# Enable the specified tables for the data agent\n",
    "for table_name in selected_tables:\n",
    "    datasource.select(table_name)\n",
    "    print(f\"   âœ“ Enabled table: {table_name}\")\n",
    "\n",
    "print(f\"âœ… Table configuration completed\")\n",
    "print(f\"   Tables available to AI: {', '.join(selected_tables)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9736bde2",
   "metadata": {},
   "source": [
    "## Step 6: Configure Data Agent with AI Instructions and Few-shot Examples\n",
    "\n",
    "Apply the AI instructions and add few-shot examples to configure the data agent's behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf96c7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Update data agent with general AI instructions\n",
    "print(f\"ðŸ¤– Updating data agent with AI instructions...\")\n",
    "print(f\"   Instructions length: {len(agent_instructions)} characters\")\n",
    "\n",
    "mgmt_client.update_configuration(instructions=agent_instructions)\n",
    "print(f\"âœ… Successfully updated data agent configuration\")\n",
    "\n",
    "# Configure data source with specific instructions and description\n",
    "print(f\"\\nðŸ”— Configuring data source instructions...\")\n",
    "print(f\"   Instructions length: {len(data_source_instructions)} characters\")\n",
    "\n",
    "datasource.update_configuration(\n",
    "    instructions=data_source_instructions,\n",
    "    user_description=data_source_description\n",
    ")\n",
    "print(f\"âœ… Successfully updated data source configuration\")\n",
    "\n",
    "# Get existing few-shot examples and remove them\n",
    "print(f\"\\nðŸ” Checking for existing few-shot examples...\")\n",
    "existing_fewshots = datasource.get_fewshots()\n",
    "print(f\"   Found {len(existing_fewshots)} existing examples\")\n",
    "\n",
    "if len(existing_fewshots) > 0:\n",
    "    print(f\"ðŸ—‘ï¸ Removing existing few-shot examples...\")\n",
    "    for i, row in existing_fewshots.iterrows():\n",
    "        fewshot_id = row['Id']\n",
    "        question = row['Question'][:50] + ('...' if len(row['Question']) > 50 else '')\n",
    "        print(f\"   Removing: {question}\")\n",
    "        datasource.remove_fewshot(fewshot_id)\n",
    "    print(f\"âœ… Successfully removed all {len(existing_fewshots)} existing examples\")\n",
    "else:\n",
    "    print(f\"   No existing examples to remove\")\n",
    "\n",
    "# Add few-shot examples to improve query generation\n",
    "print(f\"\\nðŸ“š Adding few-shot examples...\")\n",
    "print(f\"   Adding {len(fewshots_examples)} example question-query pairs\")\n",
    "\n",
    "for i, (question, query) in enumerate(fewshots_examples.items(), 1):\n",
    "    print(f\"   {i}. Adding: {question[:60]}{'...' if len(question) > 60 else ''}\")\n",
    "    single_example = {question: query}\n",
    "    datasource.add_fewshots(single_example)\n",
    "\n",
    "print(f\"âœ… Successfully added all {len(fewshots_examples)} few-shot examples\")\n",
    "\n",
    "# Verify final configuration\n",
    "fewshots_df = datasource.get_fewshots()\n",
    "config = mgmt_client.get_configuration()\n",
    "ds_config = datasource.get_configuration()\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Configuration Summary:\")\n",
    "print(f\"   Agent instructions: {'âœ“' if config.instructions else 'âœ—'}\")\n",
    "print(f\"   Data source instructions: {'âœ“' if ds_config.get('additional_instructions') else 'âœ—'}\")\n",
    "print(f\"   Data source description: {'âœ“' if ds_config.get('user_description') else 'âœ—'}\")\n",
    "print(f\"   Few-shot examples: {len(fewshots_df)}\")\n",
    "print(f\"   Datasource ID: {datasource._id}\")\n",
    "\n",
    "print(f\"\\nâœ… Data agent configuration completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529544fa",
   "metadata": {},
   "source": [
    "## Step 7: Publish Data Agent Configuration\n",
    "\n",
    "Publish the data agent configuration to make it available for use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish the data agent configuration\n",
    "print(f\"ðŸ“¤ Publishing data agent configuration...\")\n",
    "print(f\"   Making data agent available for use...\")\n",
    "\n",
    "mgmt_client.publish()\n",
    "print(f\"âœ… Successfully published data agent configuration\")\n",
    "print(f\"   Data agent is now ready to answer questions!\")\n",
    "print(f\"   You can now interact with the agent in Fabric using natural language queries\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Data Agent Configuration Complete!\")\n",
    "print(f\"   Agent ID: {data_agent_id}\")\n",
    "print(f\"   Status: Published and Ready\")\n",
    "print(f\"   Available Tables: {', '.join(selected_tables)}\")\n",
    "print(f\"   Few-shot Examples: {len(fewshots_examples)}\")\n",
    "print(f\"   Next: Test the agent with manufacturing analytics queries!\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
